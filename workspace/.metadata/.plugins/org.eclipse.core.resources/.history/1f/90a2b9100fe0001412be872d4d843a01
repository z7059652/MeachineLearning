package TestJava;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutputStream;
import java.nio.file.Files;
import java.util.Iterator;
import java.util.Set;
import java.util.TreeSet;

import com.aliasi.classify.*;
import com.aliasi.tokenizer.*;
import com.aliasi.corpus.*;

import java.util.Random;

import com.aliasi.sentences.MedlineSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunking;


/* 使用 Lingpipe的TF/IDF分类器训练语料
 * 
 * @author zhuxn
 */
public class TrainTClassifier
{

	//训练语料文件夹
	private static String[] Path  = {"D:\\visual studio 2013\\Projects\\TextDeal\\Sentiment\\cn_sample_data\\sample.positive.temp.text.nonum.key.txt","D:\\visual studio 2013\\Projects\\TextDeal\\Sentiment\\cn_sample_data\\sample.negative.temp.text.nonum.key.txt"};
	//定义分类
	private static String[] CATEGORIES = { "positive","negative" };
	private static String[] Split = {"}","Q"};
	static final TokenizerFactory TOKENIZER_FACTORY = CharacterTokenizerFactory.INSTANCE;
	static final SentenceModel SENTENCE_MODEL  = new MedlineSentenceModel();
	static final SentenceChunker SENTENCE_CHUNKER = new SentenceChunker(TOKENIZER_FACTORY,SENTENCE_MODEL);

	public static void main(String[] args) throws ClassNotFoundException,IOException 
	{
		
//		TfIdfClassifierTrainer<String> classifier = new TfIdfClassifierTrainer<String>(
//				new TokenFeatureExtractor(CharacterTokenizerFactory.INSTANCE));
//		DynamicLMClassifier classifier = DynamicLMClassifier.createNGramBoundary(CATEGORIES,2);
		Set<String> cate = new TreeSet<String>();
		cate.add(CATEGORIES[0]);
		cate.add(CATEGORIES[1]);		
		TokenizerFactory tokenfactory = CharacterTokenizerFactory.INSTANCE;
//		TradNaiveBayesClassifier classifier = new TradNaiveBayesClassifier(cate,tokenfactory);
		NaiveBayesClassifier classifier = new NaiveBayesClassifier(CATEGORIES,tokenfactory);
//		BinaryLMClassifier classifier = new BinaryLMClassifier();
        int numFolds = 10;
        XValidatingObjectCorpus<Classified<CharSequence>> corpus = new XValidatingObjectCorpus<Classified<CharSequence>>(numFolds);

		
		//读取文件 
//		String path = "D:\\visual studio 2013\\Projects\\TextDeal\\Sentiment\\cn_sample_data\\sample.negative.temp.nonum.txt";
		int[] ncout = new int[2];
		for (int i = 0; i < CATEGORIES.length; i++)
		{   
			File file = new File(Path[i]);
			if(!file.exists() || file.isDirectory())
				throw new FileNotFoundException();
			BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(Path[i]),"UTF-8"));
			String temp=null;
	        StringBuffer sb=new StringBuffer();
	        temp=br.readLine();
	        while(temp!=null)
	        {
				temp=br.readLine();
				sb.append(temp);				
	        }
	        br.close();
			Classification classification = new Classification(CATEGORIES[i]);
	        String[] strarray = sb.toString().split(Split[i]);
	        for(int j = 0;j < strarray.length;j++)
	        {
//				System.out.println("正在训练 " + CATEGORIES[i] + ":"+strarray[j]);
				Classified<CharSequence> classified = new Classified<CharSequence>(strarray[j], classification);
				classifier.handle(classified);	  
//				corpus.handle(classified);
				ncout[i]++;
	        }
		}
		// 把分类器模型写到文件上
		System.out.println("Num instances=" + corpus.size() + ".");
		System.out.println("Current fold =" + corpus.numFolds() + ".");
		corpus.permuteCorpus(new Random(7117));
		System.out.println("Num instances=" + corpus.size() + ".");
		System.out.println("Current fold =" + corpus.numFolds() + ".");

		System.out.println("开始生成分类器");
		String modelFile = "E:\\data\\category\\tclassifier";
		ObjectOutputStream os = new ObjectOutputStream(new FileOutputStream(modelFile));
		classifier.compileTo(os);
//		classifier.
//		TestTClassifier testclassifier = new TestTClassifier();
//		testclassifier.compiledClassifier = (ScoredClassifier<String>)os;
//		os.close();		
//		System.out.println(classifier.idf("垃圾"));
		System.out.println("分类器生成完成,总共训练了positive"+ncout[0]+"次,nagetive:"+ncout[1]+"次");
//		testclassifier.Test();
	}
	public static String ReadFile(String path) throws IOException
	{
		File file = new File(path);
		if(!file.exists() || file.isDirectory())
			throw new FileNotFoundException();
		BufferedReader br = new BufferedReader(new FileReader(file));
		String temp=null;
        StringBuffer sb=new StringBuffer();
        temp=br.readLine();
        while(temp!=null)
        {
            sb.append(temp+" ");
            temp=br.readLine();
        }
        br.close();
        return sb.toString();
	}
}

//Chunking chunking = SENTENCE_CHUNKER.chunk(strarray[j].toCharArray(),0,strarray[j].length());
//Set<Chunk> sentences = chunking.chunkSet();
//if (sentences.size() < 1) 
//{
//	System.out.println("No sentence chunks found.");
//	return;
//}
//String slice = chunking.charSequence().toString();
//int ii = 1;
//for (Iterator<Chunk> it = sentences.iterator(); it.hasNext(); )
//{
//	Chunk sentence = it.next();
//	int start = sentence.start();
//	int end = sentence.end();
//    System.out.println("SENTENCE "+(ii++)+":");
//    System.out.println(slice.substring(start,end));
//}